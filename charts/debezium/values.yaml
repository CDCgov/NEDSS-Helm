nameOverride: ""
fullnameOverride: ""
log_level: INFO
connect:
  replicaCount: 1
  imagePullSecrets: [ ]

  image:
    repository: quay.io/debezium/connect
    pullPolicy: IfNotPresent
    tag: "2.7"

  service:
    type: ClusterIP
    port: 8083
    protocol: TCP
    name: http

  ingress:
    enabled: false

  autoscaling:
    enabled: false

  resources:
    limits:
      cpu: 1000m
      memory: 2Gi
    requests:
      cpu: 500m
      memory: 1Gi

  properties:
    group_id: "debezium-odse-srte-connector-v072425"
    topics_basename: "debezium-odse-srte-connector-v072425"
    default_replication_factor: 2
    default_partitions: 10
    default_cleanup: "compact"
    sql_server_agent_override: false
    sql_server_agent_status: ""
    bootstrap_server: "EXAMPLE_MSK_KAFKA_ENDPOINT"

  # dbserver: "EXAMPLE_DB_ENDPOINT"

  connector_enable:
    nbs_odse: "enabled"
    nbs_srte: "enabled"

  sqlserverconnector_odse: {
    "name": "debezium-odse-connector-v072425",
    "config": {
      "connector.class": "io.debezium.connector.sqlserver.SqlServerConnector",
      "database.hostname": "nbs-db.private-EXAMPLE_DOMAIN",
      "database.port": "1433",
      "database.user": "EXAMPLE_ODSE_DB_USER",
      "database.password": "EXAMPLE_ODSE_DB_USER_PASSWORD",
      "database.dbname": "nbs_odse",
      "database.names": "nbs_odse",
      "database.server.name": "odse",
      "database.history.kafka.bootstrap.servers": "EXAMPLE_MSK_KAFKA_ENDPOINT",
      "database.history.kafka.topic": "dbhistory.database_server_name.database_name",
      # Uncomment following to manually bypass the sqlserver agent status query results
      #"database.sqlserver.agent.status.query": "select dbo.IsSqlAgentRunning()",
      "database.trustServerCertificate": "true",
      "include.schema.changes": "true",
      "key.converter": "org.apache.kafka.connect.json.JsonConverter",
      "key.converter.schemas.enable": "true",
      "producer.max.request.size": "10000000", #10MB
      "producer.message.max.bytes": "10000000", #10MB
      "snapshot.mode": "no_data",
      "schema.history.internal.kafka.topic": "odse-schema-history",
      "schema.history.internal.kafka.bootstrap.servers": "EXAMPLE_MSK_KAFKA_ENDPOINT",
      "table.include.list": "dbo.Person, dbo.Organization, dbo.Observation, dbo.Public_health_case, dbo.Treatment,
                          dbo.state_defined_field_data, dbo.Notification, dbo.Interview, 
                          dbo.Place, dbo.CT_contact, dbo.Auth_user, dbo.Intervention, dbo.Act_relationship,
                          dbo.Page_cond_mapping, dbo.NBS_page, dbo.NBS_ui_metadata,  dbo.NBS_rdb_metadata,
                          dbo.state_defined_field_metadata, dbo.NBS_configuration, dbo.LOOKUP_QUESTION",
      "tasks.max": "1",
      "topic.prefix": "cdc",
      "topic.creation.default.replication.factor": 2,
      "topic.creation.default.partitions": 10,
      "topic.creation.default.cleanup.policy": "compact",
      "time.precision.mode": "connect",
      "transforms": "dropPrefix, convertTimezone, unwrapConfig, convertTimestampsConfig_add_time, convertTimestampsConfig_last_chg_time, convertTimestampsConfig_status_time",
      "transforms.dropPrefix.regex": "cdc\\.NBS_ODSE\\.dbo\\.(.+)",
      "transforms.dropPrefix.type": "org.apache.kafka.connect.transforms.RegexRouter",
      "transforms.dropPrefix.replacement": "nbs_$1",
      "transforms.convertTimezone.type": "io.debezium.transforms.TimezoneConverter",
      "transforms.convertTimezone.converted.timezone": "UTC",
      "value.converter": "org.apache.kafka.connect.json.JsonConverter",
      "value.converter.schemas.enable": "true"
    }
  }

  sqlserverconnector_srte: {
    "name": "debezium-srte-connector-v072425",
    "config": {
      "connector.class": "io.debezium.connector.sqlserver.SqlServerConnector",
      "database.hostname": "nbs-db.private-EXAMPLE_DOMAIN",
      "database.port": "1433",
      "database.user": "EXAMPLE_ODSE_DB_USER",
      "database.password": "EXAMPLE_ODSE_DB_USER_PASSWORD",
      "database.dbname": "nbs_srte",
      "database.names": "nbs_srte",
      "database.server.name": "srte",
      "database.history.kafka.bootstrap.servers": "EXAMPLE_MSK_KAFKA_ENDPOINT",
      "database.history.kafka.topic": "dbhistory.database_server_name.database_name",
      # Uncomment following to manually bypass the sqlserver agent status query results
      #"database.sqlserver.agent.status.query": "select dbo.IsSqlAgentRunning()",
      "database.trustServerCertificate": "true",
      "include.schema.changes": "true",
      "key.converter": "org.apache.kafka.connect.json.JsonConverter",
      "key.converter.schemas.enable": "true",
      "producer.max.request.size": "10000000", #10MB
      "producer.message.max.bytes": "10000000", #10MB
      "snapshot.mode": "no_data",
      "schema.history.internal.kafka.topic": "srte-schema-history",
      "schema.history.internal.kafka.bootstrap.servers": "EXAMPLE_MSK_KAFKA_ENDPOINT",
      "table.include.list": "dbo.Condition_code,dbo.Program_area_code,dbo.Language_code,dbo.State_code,dbo.Unit_code,
                             dbo.Cntycity_code_value,dbo.Lab_result,dbo.Country_code,dbo.Labtest_loinc,dbo.ELR_XREF,
                             dbo.Loinc_condition,dbo.Loinc_snomed_condition,dbo.Lab_test,dbo.Zip_code_value,
                             dbo.Zipcnty_code_value,dbo.Lab_result_Snomed,dbo.Investigation_code,dbo.TotalIDM,
                             dbo.IMRDBMapping,dbo.Anatomic_site_code,dbo.Jurisdiction_code,dbo.Lab_coding_system,
                             dbo.City_code_value,dbo.LDF_page_set,dbo.LOINC_code,dbo.NAICS_Industry_code,
                             dbo.Codeset_Group_Metadata,dbo.Country_Code_ISO,dbo.Occupation_code,dbo.Country_XREF,
                             dbo.Standard_XREF,dbo.Code_value_clinical,dbo.Code_value_general,dbo.Race_code,dbo.Participation_type,
                             dbo.Specimen_source_code,dbo.Snomed_code, dbo.State_county_code_value,
                             dbo.State_model,dbo.Codeset, dbo.Jurisdiction_participation,dbo.Labtest_Progarea_Mapping,
                             dbo.Treatment_code, dbo.Snomed_condition",
      "tasks.max": "1",
      "topic.prefix": "cdc",
      "topic.creation.default.replication.factor": 2,
      "topic.creation.default.partitions": 10,
      "topic.creation.default.cleanup.policy": "compact",
      "time.precision.mode": "connect",
      #      "transforms": "dropPrefix, convertTimezone, unwrap, convertTimestamps_add_time, convertTimestamps_concept_status_time,
      #                     convertTimestamps_effective_from_time, convertTimestamps_effective_to_time, convertTimestamps_status_time,
      #                     convertTimestamps_status_to_time, convertTimestamps_value_set_status_time",
      "transforms": "dropPrefix, dropPrefixConfig, unwrapConfig, valueToKey, convertTimezone, unwrap, 
                     convertTimestamps_add_time, convertTimestamps_concept_status_time, convertTimestamps_effective_from_time,
                     convertTimestamps_effective_to_time, convertTimestamps_status_time, convertTimestamps_status_to_time,
                     convertTimestamps_value_set_status_time, convertTimestampsConfig_effective_from_time,
                     convertTimestampsConfig_effective_to_time, convertTimestampsConfig_status_time",
      "predicates": "isConfigTable",
      "transforms.dropPrefix.regex": "cdc\\.NBS_SRTE\\.dbo\\.(.+)",
      "transforms.dropPrefix.type": "org.apache.kafka.connect.transforms.RegexRouter",
      "transforms.dropPrefix.replacement": "nrt_srte_$1",

      ### Snomed condition specific transforms
      "transforms.dropPrefixConfig.regex": "cdc\\.NBS_SRTE\\.dbo\\.Snomed_condition",
      "transforms.dropPrefixConfig.type": "org.apache.kafka.connect.transforms.RegexRouter",
      "transforms.dropPrefixConfig.replacement": "nrt_srte_Snomed_condition",
      "transforms.unwrapConfig.type": "io.debezium.transforms.ExtractNewRecordState",
      "transforms.unwrapConfig.predicate": "isConfigTable",
      "transforms.valueToKey.type": "org.apache.kafka.connect.transforms.ValueToKey",
      "transforms.valueToKey.fields": "snomed_cd",
      "transforms.valueToKey.predicate": "isConfigTable",
      ######

      "transforms.convertTimezone.type": "io.debezium.transforms.TimezoneConverter",
      "transforms.convertTimezone.converted.timezone": "UTC",
      "transforms.unwrap.type": "io.debezium.transforms.ExtractNewRecordState",
      "transforms.convertTimestamps_add_time.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
      "transforms.convertTimestamps_add_time.target.type": "string",
      "transforms.convertTimestamps_add_time.format": "yyyy-MM-dd HH:mm:ss.SSS",
      "transforms.convertTimestamps_add_time.field": "add_time",
      "transforms.convertTimestamps_concept_status_time.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
      "transforms.convertTimestamps_concept_status_time.target.type": "string",
      "transforms.convertTimestamps_concept_status_time.format": "yyyy-MM-dd HH:mm:ss.SSS",
      "transforms.convertTimestamps_concept_status_time.field": "concept_status_time",
      "transforms.convertTimestamps_effective_from_time.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
      "transforms.convertTimestamps_effective_from_time.target.type": "string",
      "transforms.convertTimestamps_effective_from_time.format": "yyyy-MM-dd HH:mm:ss.SSS",
      "transforms.convertTimestamps_effective_from_time.field": "effective_from_time",
      "transforms.convertTimestamps_effective_to_time.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
      "transforms.convertTimestamps_effective_to_time.target.type": "string",
      "transforms.convertTimestamps_effective_to_time.format": "yyyy-MM-dd HH:mm:ss.SSS",
      "transforms.convertTimestamps_effective_to_time.field": "effective_to_time",
      "transforms.convertTimestamps_status_time.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
      "transforms.convertTimestamps_status_time.target.type": "string",
      "transforms.convertTimestamps_status_time.format": "yyyy-MM-dd HH:mm:ss.SSS",
      "transforms.convertTimestamps_status_time.field": "status_time",
      "transforms.convertTimestamps_status_to_time.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
      "transforms.convertTimestamps_status_to_time.target.type": "string",
      "transforms.convertTimestamps_status_to_time.format": "yyyy-MM-dd HH:mm:ss.SSS",
      "transforms.convertTimestamps_status_to_time.field": "status_to_time",
      "transforms.convertTimestamps_value_set_status_time.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
      "transforms.convertTimestamps_value_set_status_time.target.type": "string",
      "transforms.convertTimestamps_value_set_status_time.format": "yyyy-MM-dd HH:mm:ss.SSS",
      "transforms.convertTimestamps_value_set_status_time.field": "value_set_status_time",

      #Predicate transforms
      "transforms.convertTimestampsConfig_effective_from_time.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
      "transforms.convertTimestampsConfig_effective_from_time.target.type": "string",
      "transforms.convertTimestampsConfig_effective_from_time.format": "yyyy-MM-dd HH:mm:ss.SSS",
      "transforms.convertTimestampsConfig_effective_from_time.field": "effective_from_time",
      "transforms.convertTimestampsConfig_effective_from_time.predicate": "isConfigTable",
      "transforms.convertTimestampsConfig_effective_to_time.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
      "transforms.convertTimestampsConfig_effective_to_time.target.type": "string",
      "transforms.convertTimestampsConfig_effective_to_time.format": "yyyy-MM-dd HH:mm:ss.SSS",
      "transforms.convertTimestampsConfig_effective_to_time.field": "effective_to_time",
      "transforms.convertTimestampsConfig_effective_to_time.predicate": "isConfigTable",
      "transforms.convertTimestampsConfig_status_time.type": "org.apache.kafka.connect.transforms.TimestampConverter$Value",
      "transforms.convertTimestampsConfig_status_time.target.type": "string",
      "transforms.convertTimestampsConfig_status_time.format": "yyyy-MM-dd HH:mm:ss.SSS",
      "transforms.convertTimestampsConfig_status_time.field": "status_time",
      "transforms.convertTimestampsConfig_status_time.predicate": "isConfigTable",
      "predicates.isConfigTable.type": "org.apache.kafka.connect.transforms.predicates.TopicNameMatches",
      "predicates.isConfigTable.pattern": "nrt_srte_Snomed_condition",
      ####

      "value.converter": "org.apache.kafka.connect.json.JsonConverter",
      "value.converter.schemas.enable": "true",
    }
  }

  env:
    - name: BOOTSTRAP_SERVERS
      value: "EXAMPLE_MSK_KAFKA_ENDPOINT"
    - name: LOG_LEVEL
      value: "INFO"
    - name: KAFKA_LOG4J_OPTS
      value: "-Dlog4j.configuration=file:/kafka/config/log4j.properties"
    - name: NAME
      value: "debezium-odse-srte-connector-v072425"
    - name: TZ
      value: "UTC"

ui:
  enabled: false
  replicaCount: 1
  imagePullSecrets: [ ]

  image:
    repository: debezium/debezium-ui
    pullPolicy: Always
    tag: "2.7"

  service:
    type: ClusterIP
    port: "8080"
    protocol: TCP
    name: http

  ingress:
    enabled: true
    router: private
    host: EXAMPLE_FIXME_DEBEZIUM_UI_HOST_ADDRESS

  autoscaling:
    enabled: false

  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 500m
      memory: 1Gi

  env:
    - name: KAFKA_CONNECT_URIS
      value: "http://debezium-connect:8083"
    - name: TZ
      value: "UTC"

